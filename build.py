import requests
from bs4 import BeautifulSoup
import os
import json

KEYWORD_FILE = "inSys/keywords.txt"
ENTRIES_DIR = "entries"
OUTPUT_JSON = "inSys/entry_list.json"

def fetch_namu_page(keyword, idx):

        url = f"https://namu.moe/w/{keyword}"
        wiki_url = f"https://namu.wiki/w/{keyword}"  # ì´ê±´ ê³µì‹ ìœ„í‚¤ìš© ë§í¬
        char_url = f"https://namu.wiki/w/{keyword}(íŠ¸ë¦­ì»¬%20ë¦¬ë°”ì´ë¸Œ)" # ìºë¦­í„°ìš© ë¬¸ì„œ
        headers = {
            "User-Agent": "Mozilla/5.0",
            "Referer": "https://namu.moe/",
            "Accept-Language": "ko-KR,ko;q=0.9"
        }
    
        try:
            res = requests.get(url, headers=headers)
            
            if res.status_code != 200:
                print(f"[!] '{keyword}' ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (status: {res.status_code})")
                return False

            soup = BeautifulSoup(res.text, "html.parser")
            article = soup.find("article")
            summary = article.find("p").text.strip() if article and article.find("p") else "ìš”ì•½ ì •ë³´ ì—†ìŒ"
    
            code = f"{idx+1:04}"  # 0001, 0002, ...
            html_content = f"""
            <!DOCTYPE html>
            <html lang="ko">
            <head><meta charset="UTF-8"><title>{keyword}</title></head>
            <body>
            <h1>{keyword}</h1>
            <p>{summary}</p>
            <p><strong>ì½”ë“œ: code[{code}]</strong></p>
            <p><a href="{url}" target="_blank">[ë‚˜ë¬´ìœ„í‚¤ ì›ë¬¸ ë³´ê¸°]</a></p>
            </body></html>
            """
    
            with open(os.path.join(ENTRIES_DIR, f"{keyword}.html"), "w", encoding="utf-8") as f:
                f.write(html_content)
    
            print(f"[âœ“] '{keyword}' ì €ì¥ ì™„ë£Œ")
            return True
    
        except Exception as e:
            print(f"[!] '{keyword}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

def generate_entry_list():
    files = os.listdir(ENTRIES_DIR)
    entries = [f.replace(".html", "") for f in files if f.endswith(".html")]

    # âœ… ì—¬ê¸°ì„œ ì •ë ¬ ì¶”ê°€!
    entries.sort()  # ê°€ë‚˜ë‹¤ ë˜ëŠ” code[0001] ìˆœì„œë¡œ ì •ë ¬ë¨

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(entries, f, ensure_ascii=False, indent=2)

    print(f"[âœ“] entry_list.json ìƒì„± ì™„ë£Œ ({len(entries)}ê°œ í•­ëª©)")


def main():
    os.makedirs(ENTRIES_DIR, exist_ok=True)

    if not os.path.exists(KEYWORD_FILE):
        print(f"[X] '{KEYWORD_FILE}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    with open(KEYWORD_FILE, "r", encoding="utf-8") as f:
        keywords = [line.strip() for line in f if line.strip()]
        
    # del
    # for kw in keywords:
    #     fetch_namu_page(kw)

    for idx, kw in enumerate(keywords):
        fetch_namu_page(kw, idx)

    generate_entry_list()
    print("[ğŸ‰] ìë™ ë¹Œë“œ ì™„ë£Œ!")

if __name__ == "__main__":
    main()

